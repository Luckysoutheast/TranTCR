{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5bf118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "random.seed(1234)\n",
    "from scipy import interp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from Bio.Align import substitution_matrices\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from tqdm import tqdm, trange\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658a55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    '''\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
    "    '''\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], False is masked\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n",
    "#         attn = scores\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        '''\n",
    "        input_Q: [batch_size, len_q, d_model]\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_V: [batch_size, len_v(=len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        '''\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n",
    "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v]\n",
    "        output = self.fc(context) # [batch_size, len_q, d_model]\n",
    "        return nn.LayerNorm(d_model).to(device)(output + residual), attn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model, bias=False)\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        inputs: [batch_size, seq_len, d_model]\n",
    "        '''\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        return nn.LayerNorm(d_model).to(device)(output + residual) # [batch_size, seq_len, d_model]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len]\n",
    "        '''\n",
    "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        '''\n",
    "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model]\n",
    "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) # [batch_size, src_len, src_len]\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "\n",
    "# ### Decoder\n",
    "\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, dec_self_attn_mask): # dec_inputs = enc_outputs\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        '''\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs) # [batch_size, tgt_len, d_model]\n",
    "        return dec_outputs, dec_self_attn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "#         self.tgt_emb = nn.Embedding(d_model * 2, d_model)\n",
    "        self.use_cuda = use_cuda\n",
    "        device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "        self.tgt_len = tgt_len\n",
    "        \n",
    "    def forward(self, dec_inputs): # dec_inputs = enc_outputs (batch_size, peptide_hla_maxlen_sum, d_model)\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_intpus: [batch_size, src_len]\n",
    "        enc_outputs: [batsh_size, src_len, d_model]\n",
    "        '''\n",
    "#         dec_outputs = self.tgt_emb(dec_inputs) # [batch_size, tgt_len, d_model]\n",
    "        dec_outputs = self.pos_emb(dec_inputs.transpose(0, 1)).transpose(0, 1).to(device) # [batch_size, tgt_len, d_model]\n",
    "#         dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]\n",
    "        dec_self_attn_pad_mask = torch.LongTensor(np.zeros((dec_inputs.shape[0], tgt_len, tgt_len))).bool().to(device)\n",
    " \n",
    "        dec_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "            dec_outputs, dec_self_attn = layer(dec_outputs, dec_self_attn_pad_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            \n",
    "        return dec_outputs, dec_self_attns\n",
    "\n",
    "\n",
    "# ### Transformer\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.pep_encoder = Encoder().to(device)\n",
    "        self.hla_encoder = Encoder().to(device)\n",
    "        self.decoder = Decoder().to(device)\n",
    "        self.tgt_len = tgt_len\n",
    "        self.projection = nn.Sequential(\n",
    "                                        nn.Linear(tgt_len * d_model, 256),\n",
    "                                        nn.ReLU(True),\n",
    "\n",
    "                                        nn.BatchNorm1d(256),\n",
    "                                        nn.Linear(256, 64),\n",
    "                                        nn.ReLU(True),\n",
    "\n",
    "                                        #output layer\n",
    "                                        nn.Linear(64, 2)\n",
    "                                        ).to(device)\n",
    "        \n",
    "    def forward(self, hla_inputs,pep_inputs):\n",
    "        '''\n",
    "        pep_inputs: [batch_size, pep_len]\n",
    "        hla_inputs: [batch_size, hla_len]\n",
    "        '''\n",
    "        # tensor to store decoder outputs\n",
    "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\n",
    "        hla_enc_outputs, hla_enc_self_attns = self.hla_encoder(hla_inputs)\n",
    "        pep_enc_outputs, pep_enc_self_attns = self.pep_encoder(pep_inputs)\n",
    "        \n",
    "#         print(hla_enc_outputs)\n",
    "        enc_outputs = torch.cat((hla_enc_outputs,pep_enc_outputs), 1) # concat pep & hla embedding\n",
    "        ## reverse ##\n",
    "#         enc_outputs = pep_enc_outputs*hla_enc_outputs\n",
    "        \n",
    "        ## end ##\n",
    "        # dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\n",
    "        dec_outputs, dec_self_attns = self.decoder(enc_outputs)\n",
    "        dec_outputs = dec_outputs.view(dec_outputs.shape[0], -1) # Flatten [batch_size, tgt_len * d_model]\n",
    "        dec_logits = self.projection(dec_outputs) # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), pep_enc_self_attns, hla_enc_self_attns, dec_self_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6902b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pep_max_len = 12 # peptide; enc_input max sequence length\n",
    "hla_max_len = 20 # hla; dec_input(=dec_output) max sequence length\n",
    "tgt_len = pep_max_len + hla_max_len\n",
    "pep_max_len, hla_max_len\n",
    "vocab_size = 21\n",
    "d_model=64 # Embedding Size\n",
    "d_ff = 256 # FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_layers = 1  # number of Encoder of Decoder Layer\n",
    "n_heads = 9\n",
    "batch_size = 1024\n",
    "# batch_size = 5000\n",
    "epochs = 150\n",
    "threshold = 0.5\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4fd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performances(y_true, y_pred, y_prob, print_ = True):\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel().tolist()\n",
    "    accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "    try:\n",
    "        mcc = ((tp*tn) - (fn*fp)) / np.sqrt(np.float((tp+fn)*(tn+fp)*(tp+fp)*(tn+fn)))\n",
    "    except:\n",
    "        print('MCC Error: ', (tp+fn)*(tn+fp)*(tp+fp)*(tn+fn))\n",
    "        mcc = np.nan\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    \n",
    "    try:\n",
    "        recall = tp / (tp+fn)\n",
    "    except:\n",
    "        recall = np.nan\n",
    "        \n",
    "    try:\n",
    "        precision = tp / (tp+fp)\n",
    "    except:\n",
    "        precision = np.nan\n",
    "        \n",
    "    try: \n",
    "        f1 = 2*precision*recall / (precision+recall)\n",
    "    except:\n",
    "        f1 = np.nan\n",
    "        \n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    prec, reca, _ = precision_recall_curve(y_true, y_prob)\n",
    "    aupr = auc(reca, prec)\n",
    "    \n",
    "    if print_:\n",
    "        print('tn = {}, fp = {}, fn = {}, tp = {}'.format(tn, fp, fn, tp))\n",
    "        print('y_pred: 0 = {} | 1 = {}'.format(Counter(y_pred)[0], Counter(y_pred)[1]))\n",
    "        print('y_true: 0 = {} | 1 = {}'.format(Counter(y_true)[0], Counter(y_true)[1]))\n",
    "        print('auc={:.4f}|sensitivity={:.4f}|specificity={:.4f}|acc={:.4f}|mcc={:.4f}'.format(roc_auc, sensitivity, specificity, accuracy, mcc))\n",
    "        print('precision={:.4f}|recall={:.4f}|f1={:.4f}|aupr={:.4f}'.format(precision, recall, f1, aupr))\n",
    "        \n",
    "    return (roc_auc, accuracy, mcc, f1, sensitivity, specificity, precision, recall, aupr)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "def transfer(y_prob, threshold = 0.5):\n",
    "    # return np.array([[0, 1][x > threshold] for x in y_prob])\n",
    "    y_prob = np.array(y_prob)\n",
    "    return np.where(y_prob > threshold, 1, 0)\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "f_mean = lambda l: sum(l)/len(l)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "def performances_to_pd(performances_list):\n",
    "    metrics_name = ['roc_auc', 'accuracy', 'mcc', 'f1', 'sensitivity', 'specificity', 'precision', 'recall', 'aupr']\n",
    "\n",
    "    performances_pd = pd.DataFrame(performances_list, columns = metrics_name)\n",
    "    performances_pd.loc['mean'] = performances_pd.mean(axis = 0)\n",
    "    performances_pd.loc['std'] = performances_pd.std(axis = 0)\n",
    "    \n",
    "    return performances_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a4e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, train_loader, fold, epoch, epochs, use_cuda = True):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    time_train_ep = 0\n",
    "    model.train()\n",
    "    y_true_train_list, y_prob_train_list = [], []\n",
    "    loss_train_list, dec_attns_train_list = [], []\n",
    "    for train_pep_inputs, train_hla_inputs, train_labels in tqdm(train_loader):\n",
    "        '''\n",
    "        pep_inputs: [batch_size, pep_len]\n",
    "        hla_inputs: [batch_size, hla_len]\n",
    "        train_outputs: [batch_size, 2]\n",
    "        '''\n",
    "        train_pep_inputs, train_hla_inputs, train_labels = train_pep_inputs.to(device), train_hla_inputs.to(device), train_labels.to(device)\n",
    "#         print(train_pep_inputs.shape,train_hla_inputs.shape)\n",
    "        t1 = time.time()\n",
    "        train_outputs, _, _, train_dec_self_attns = model(train_hla_inputs, train_pep_inputs)\n",
    "        train_loss = criterion(train_outputs, train_labels)\n",
    "        time_train_ep += time.time() - t1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_true_train = train_labels.cpu().numpy()\n",
    "        y_prob_train = nn.Softmax(dim = 1)(train_outputs)[:, 1].cpu().detach().numpy()\n",
    "        \n",
    "        y_true_train_list.extend(y_true_train)\n",
    "        y_prob_train_list.extend(y_prob_train)\n",
    "        loss_train_list.append(train_loss)\n",
    "#         dec_attns_train_list.append(train_dec_self_attns)\n",
    "        \n",
    "    y_pred_train_list = transfer(y_prob_train_list, threshold)\n",
    "    ys_train = (y_true_train_list, y_pred_train_list, y_prob_train_list)\n",
    "    \n",
    "    print('Fold-{}****Train (Ep avg): Epoch-{}/{} | Loss = {:.4f} | Time = {:.4f} sec'.format(fold, epoch, epochs, f_mean(loss_train_list), time_train_ep))\n",
    "    metrics_train = performances(y_true_train_list, y_pred_train_list, y_prob_train_list, print_ = True)\n",
    "    \n",
    "    return ys_train, loss_train_list, metrics_train, time_train_ep#, dec_attns_train_list\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "def eval_step(model, val_loader, fold, epoch, epochs, use_cuda = True):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    torch.manual_seed(19961231)\n",
    "    torch.cuda.manual_seed(19961231)\n",
    "    with torch.no_grad():\n",
    "        loss_val_list, dec_attns_val_list = [], []\n",
    "        y_true_val_list, y_prob_val_list = [], []\n",
    "        for val_pep_inputs, val_hla_inputs, val_labels in tqdm(val_loader):\n",
    "            val_pep_inputs, val_hla_inputs, val_labels = val_pep_inputs.to(device), val_hla_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs, _, _, val_dec_self_attns = model(val_hla_inputs,val_pep_inputs)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            y_true_val = val_labels.cpu().numpy()\n",
    "            y_prob_val = nn.Softmax(dim = 1)(val_outputs)[:, 1].cpu().detach().numpy()\n",
    "\n",
    "            y_true_val_list.extend(y_true_val)\n",
    "            y_prob_val_list.extend(y_prob_val)\n",
    "            loss_val_list.append(val_loss)\n",
    "#             dec_attns_val_list.append(val_dec_self_attns)\n",
    "            \n",
    "        y_pred_val_list = transfer(y_prob_val_list, threshold)\n",
    "        ys_val = (y_true_val_list, y_pred_val_list, y_prob_val_list)\n",
    "        \n",
    "        print('Fold-{} ****Test  Epoch-{}/{}: Loss = {:.6f}'.format(fold, epoch, epochs, f_mean(loss_val_list)))\n",
    "        metrics_val = performances(y_true_val_list, y_pred_val_list, y_prob_val_list, print_ = True)\n",
    "    return ys_val, loss_val_list, metrics_val#, dec_attns_val_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a372af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(data):\n",
    "#     labels = []\n",
    "    cdr3 = data['CDR3'].values\n",
    "    epitope = data['Epitope'].values\n",
    "    labels = data['label'].values\n",
    "    mat = Tokenizer() \n",
    "    hla_inputs = encode_cdr3(cdr3, mat)\n",
    "#     print(hla_inputs)\n",
    "    pep_inputs = encode_epi(epitope,mat)\n",
    "#     epi_encoder = PretrainedEncoder(mat)\n",
    "#     pep_inputs, epi_vec = epi_encoder.encode_pretrained_epi(epitope)\n",
    "#     labels.append(label)\n",
    "    return torch.LongTensor(pep_inputs), torch.LongTensor(hla_inputs), torch.LongTensor(labels)\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, pep_inputs, hla_inputs,labels):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.pep_inputs = pep_inputs\n",
    "        self.hla_inputs = hla_inputs\n",
    "        self.labels = labels\n",
    "        \n",
    "\n",
    "    def __len__(self): # 样本数\n",
    "        return self.pep_inputs.shape[0] # 改成hla_inputs也可以哦！\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         return self.pep_inputs[idx], self.hla_inputs[idx], self.labels[idx],self.pep_lens[idx]\n",
    "        return self.pep_inputs[idx],self.hla_inputs[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "176f393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "\n",
    "def GetBlosumMat(residues_list):\n",
    "    n_residues = len(residues_list)  # the number of amino acids _ 'X'\n",
    "    blosum62_mat = np.zeros([n_residues, n_residues])  # plus 1 for gap\n",
    "    bl_dict = substitution_matrices.load('BLOSUM62')\n",
    "    for pair, score in bl_dict.items():\n",
    "        if (pair[0] not in residues_list) or (pair[1] not in residues_list):  # special residues not considered here\n",
    "            continue\n",
    "        idx_pair0 = residues_list.index(pair[0])  # index of residues\n",
    "        idx_pair1 = residues_list.index(pair[1])\n",
    "        blosum62_mat[idx_pair0, idx_pair1] = score\n",
    "        blosum62_mat[idx_pair1, idx_pair0] = score\n",
    "    return blosum62_mat\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self,):\n",
    "        self.res_all = ['G', 'A', 'V', 'L', 'I', 'F', 'W', 'Y', 'D', 'N',\n",
    "                     'E', 'K', 'Q', 'M', 'S', 'T', 'C', 'P', 'H', 'R'] #+ ['X'] #BJZOU\n",
    "        self.tokens = ['-'] + self.res_all # '-' for padding encoding\n",
    "\n",
    "    def tokenize(self, index): # int 2 str\n",
    "        return self.tokens[index]\n",
    "\n",
    "    def id(self, token): # str 2 int\n",
    "        try:\n",
    "            return self.tokens.index(token.upper())\n",
    "        except ValueError:\n",
    "            print('Error letter in the sequences:', token)\n",
    "            if str.isalpha(token):\n",
    "                return self.tokens.index('X')\n",
    "\n",
    "    def tokenize_list(self, seq):\n",
    "        return [self.tokenize(i) for i in seq]\n",
    "\n",
    "    def id_list(self, seq):\n",
    "        return [self.id(s) for s in seq]\n",
    "\n",
    "    def embedding_mat(self):\n",
    "        blosum62 = GetBlosumMat(self.res_all)\n",
    "        mat = np.eye(len(self.tokens))\n",
    "        mat[1:len(self.res_all) + 1, 1:len(self.res_all) + 1] = blosum62\n",
    "        return mat\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "def encoding_epi(seqs, max_len=12):\n",
    "    encoding = np.zeros([len(seqs), max_len], dtype='long')\n",
    "    for i, seq in tqdm(enumerate(seqs), desc='Encoding epi seqs', total=len(seqs)):\n",
    "        len_seq = len(seq)\n",
    "        if len_seq == 8:\n",
    "            encoding[i, 2:len_seq+2] = tokenizer.id_list(seq)\n",
    "        elif (len_seq == 9) or (len_seq == 10):\n",
    "            encoding[i, 1:len_seq+1] = tokenizer.id_list(seq)\n",
    "        else:\n",
    "            encoding[i, :len_seq] = tokenizer.id_list(seq)\n",
    "    return encoding\n",
    "\n",
    "def encoding_cdr3(seqs, max_len=20):\n",
    "    encoding = np.zeros([len(seqs), max_len], dtype='long')\n",
    "    for i, seq in tqdm(enumerate(seqs), desc='Encoding cdr3s', total=len(seqs)):\n",
    "        len_seq = len(seq)\n",
    "        i_start =  max_len // 2 - len_seq // 2\n",
    "        encoding[i, i_start:i_start+len_seq] = tokenizer.id_list(seq)\n",
    "    return encoding\n",
    "\n",
    "def encoding_cdr3_single(seq, max_len=20):\n",
    "    encoding = np.zeros(max_len, dtype='long')\n",
    "    len_seq = len(seq)\n",
    "    i_start =  max_len // 2 - len_seq // 2\n",
    "    encoding[i_start:i_start+len_seq] = tokenizer.id_list(seq)\n",
    "    return encoding\n",
    "\n",
    "def encoding_epi_single(seq, max_len=12):\n",
    "    encoding = np.zeros(max_len, dtype='long')\n",
    "    len_seq = len(seq)\n",
    "    if len_seq == 8:\n",
    "        encoding[2:len_seq+2] = tokenizer.id_list(seq)\n",
    "    elif (len_seq == 9) or (len_seq == 10):\n",
    "        encoding[1:len_seq+1] = tokenizer.id_list(seq)\n",
    "    else:\n",
    "        encoding[:len_seq] = tokenizer.id_list(seq)\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def encoding_dist_mat(mat_list, max_cdr3=20, max_epi=12):\n",
    "    encoding = np.zeros([len(mat_list), max_cdr3, max_epi], dtype='float32')\n",
    "    masking = np.zeros([len(mat_list), max_cdr3, max_epi], dtype='bool')\n",
    "    for i, mat in tqdm(enumerate(mat_list), desc='Encoding dist mat', total=len(mat_list)):\n",
    "        len_cdr3, len_epi = mat.shape\n",
    "        i_start_cdr3 = max_cdr3 // 2 - len_cdr3 // 2\n",
    "        if len_epi == 8:\n",
    "            i_start_epi = 2\n",
    "        elif (len_epi == 9) or (len_epi == 10):\n",
    "            i_start_epi = 1\n",
    "        else:\n",
    "            i_start_epi = 0\n",
    "        encoding[i, i_start_cdr3:i_start_cdr3+len_cdr3, i_start_epi:i_start_epi+len_epi] = mat\n",
    "        masking[i, i_start_cdr3:i_start_cdr3+len_cdr3, i_start_epi:i_start_epi+len_epi] = True\n",
    "    return encoding, masking\n",
    "\n",
    "\n",
    "def decoding_one_mat(mat, len_cdr3, len_epi):\n",
    "    decoding = np.zeros([len_cdr3, len_epi] + list(mat.shape[2:]), dtype=mat.dtype)\n",
    "    i_start_cdr3 = 10 - len_cdr3 // 2\n",
    "    if len_epi == 8:\n",
    "        i_start_epi = 2\n",
    "    elif (len_epi == 9) or (len_epi == 10):\n",
    "        i_start_epi = 1\n",
    "    else:\n",
    "        i_start_epi = 0\n",
    "    decoding = mat[i_start_cdr3:i_start_cdr3+len_cdr3, i_start_epi:i_start_epi+len_epi] \n",
    "    return decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc8d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Align import substitution_matrices\n",
    "def GetBlosumMat(residues_list):\n",
    "    n_residues = len(residues_list)  # the number of amino acids _ 'X'\n",
    "    blosum62_mat = np.zeros([n_residues, n_residues])  # plus 1 for gap\n",
    "    bl_dict = substitution_matrices.load('BLOSUM62')\n",
    "    for pair, score in bl_dict.items():\n",
    "        if (pair[0] not in residues_list) or (pair[1] not in residues_list):  # special residues not considered here\n",
    "            continue\n",
    "        idx_pair0 = residues_list.index(pair[0])  # index of residues\n",
    "        idx_pair1 = residues_list.index(pair[1])\n",
    "        blosum62_mat[idx_pair0, idx_pair1] = score\n",
    "        blosum62_mat[idx_pair1, idx_pair0] = score\n",
    "    return blosum62_mat\n",
    "class Tokenizer:\n",
    "    def __init__(self,):\n",
    "        self.res_all = ['G', 'A', 'V', 'L', 'I', 'F', 'W', 'Y', 'D', 'N',\n",
    "                     'E', 'K', 'Q', 'M', 'S', 'T', 'C', 'P', 'H', 'R'] #+ ['X'] #BJZOU\n",
    "        self.tokens = ['-'] + self.res_all # '-' for padding encoding\n",
    "\n",
    "    def tokenize(self, index): # int 2 str\n",
    "        return self.tokens[index]\n",
    "\n",
    "    def id(self, token): # str 2 int\n",
    "        try:\n",
    "            return self.tokens.index(token.upper())\n",
    "        except ValueError:\n",
    "            print('Error letter in the sequences:', token)\n",
    "            if str.isalpha(token):\n",
    "                return self.tokens.index('X')\n",
    "\n",
    "    def tokenize_list(self, seq):\n",
    "        return [self.tokenize(i) for i in seq]\n",
    "\n",
    "    def id_list(self, seq):\n",
    "        return [self.id(s) for s in seq]\n",
    "\n",
    "    def embedding_mat(self):\n",
    "        blosum62 = GetBlosumMat(self.res_all)\n",
    "        mat = np.eye(len(self.tokens))\n",
    "        mat[1:len(self.res_all) + 1, 1:len(self.res_all) + 1] = blosum62\n",
    "        return mat\n",
    "def encode_cdr3(cdr3, tokenizer):\n",
    "    len_cdr3 = [len(s) for s in cdr3]\n",
    "    max_len_cdr3 = np.max(len_cdr3)\n",
    "    assert max_len_cdr3 <= 20, 'The cdr3 length must <= 20'\n",
    "    max_len_cdr3 = 20\n",
    "    \n",
    "    seqs_al = get_numbering(cdr3)\n",
    "    num_samples = len(seqs_al)\n",
    "\n",
    "    # encoding\n",
    "    encoding_cdr3 = np.zeros([num_samples, max_len_cdr3], dtype='int32')\n",
    "    for i, seq in enumerate(seqs_al):\n",
    "        encoding_cdr3[i, ] = tokenizer.id_list(seq)\n",
    "    return encoding_cdr3\n",
    "# def encode_epi(epi, tokenizer):\n",
    "#     # tokenizer = Tokenizer()\n",
    "#     encoding_epi = np.zeros([12], dtype='int32')\n",
    "#     len_epi = len(epi)\n",
    "#     if len_epi == 8:\n",
    "#         encoding_epi[2:len_epi+2] = tokenizer.id_list(epi)\n",
    "#     elif (len_epi == 9) or (len_epi == 10):\n",
    "#         encoding_epi[1:len_epi+1] = tokenizer.id_list(epi)\n",
    "#     else:\n",
    "#         encoding_epi[:len_epi] = tokenizer.id_list(epi)\n",
    "#     return encoding_epi\n",
    "\n",
    "def encode_epi(epi, tokenizer):\n",
    "    tokenizer = Tokenizer()\n",
    "    encoding_epi = np.zeros([len(epi),12], dtype='int32')\n",
    "    for i, seq in enumerate(epi):\n",
    "        len_epi = len(seq)\n",
    "        \n",
    "        if len_epi == 8:\n",
    "        \n",
    "            encoding_epi[i,2:len_epi+2] = tokenizer.id_list(seq)\n",
    "        elif (len_epi == 9) or (len_epi == 10) or (len_epi ==11):\n",
    "            \n",
    "            encoding_epi[i,1:len_epi+1] = tokenizer.id_list(seq)\n",
    "        else:\n",
    "            \n",
    "            encoding_epi[i,:len_epi] = tokenizer.id_list(seq)\n",
    "    print(encoding_epi)\n",
    "    return encoding_epi\n",
    "\n",
    "def get_numbering(seqs, ):\n",
    "    \"\"\"\n",
    "    get the IMGT numbering of CDR3 with ANARCI tool\n",
    "    \"\"\"\n",
    "    template = ['GVTQTPKFQVLKTGQSMTLQCAQDMNHEYMSWYRQDPGMGLRLIHYSVGAGTTDQGEVPNGYNVSRSTIEDFPLRLLSAAPSQTSVYF', 'GEGSRLTVL']\n",
    "    # # save fake tcr file\n",
    "    save_path = 'tmp_faketcr.fasta'\n",
    "    id_list = []\n",
    "    seqs_uni = np.unique(seqs)\n",
    "    with open(save_path, 'w+') as f:\n",
    "        for i, seq in enumerate(seqs_uni):\n",
    "            f.write('>'+str(i)+'\\n')\n",
    "            id_list.append(i)\n",
    "            total_seq = ''.join([template[0], seq ,template[1]])\n",
    "            f.write(str(total_seq))\n",
    "            f.write('\\n')\n",
    "    print('Save fasta file to '+save_path + '\\n Aligning...')\n",
    "    df_seqs = pd.DataFrame(list(zip(id_list, seqs_uni)), columns=['Id', 'cdr3'])\n",
    "    \n",
    "    # # using ANARCI to get numbering file\n",
    "\n",
    "   # this environment name should be the same as the one you install anarci\n",
    "    !ANARCI -i ./tmp_faketcr.fasta  -o tmp_align --csv -p 24\n",
    "#     res = os.system(cmd)\n",
    "    \n",
    "    # # parse numbered seqs data\n",
    "    try:\n",
    "        df = pd.read_csv('tmp_align_B.csv')\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError('Error: ANARCI failed to align, please check whether ANARCI exists in your environment')\n",
    "        \n",
    "    cols = ['104', '105', '106', '107', '108', '109', '110', '111', '111A', '111B', '112C', '112B', '112A', '112', '113', '114', '115', '116', '117', '118']\n",
    "    seqs_al = []\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            seqs_al_curr = df[col].values\n",
    "            seqs_al.append(seqs_al_curr)\n",
    "        else:\n",
    "            seqs_al_curr = np.full([len(df)], '-')\n",
    "            seqs_al.append(seqs_al_curr)\n",
    "    seqs_al = [''.join(seq) for seq in np.array(seqs_al).T]\n",
    "    df_al = df[['Id']]\n",
    "    df_al['cdr3_align'] = seqs_al\n",
    "    \n",
    "    ## merge\n",
    "    # os.remove('tmp_align_B.csv')\n",
    "#     os.remove('tmp_faketcr.fasta')\n",
    "    df = df_seqs.merge(df_al, how='inner', on='Id')\n",
    "    df = df.set_index('cdr3')\n",
    "    return df.loc[seqs, 'cdr3_align'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4231516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class View(nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "    def forward(self, input):\n",
    "        shape = [input.shape[0]] + list(self.shape)\n",
    "        return input.view(*shape)\n",
    "def load_ae_model(tokenizer, path='./epi_ae.ckpt',):\n",
    "    # tokenizer = Tokenizer()\n",
    "    ## load model\n",
    "    model_args = dict(\n",
    "        tokenizer = tokenizer,\n",
    "        dim_hid = 32,\n",
    "        len_seq = 12,\n",
    "    )\n",
    "    model = AutoEncoder(**model_args)\n",
    "    model.eval()\n",
    "\n",
    "    ## load weights\n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    state_dict = {k[6:]:v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "        tokenizer,\n",
    "        dim_hid,\n",
    "        len_seq,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        embedding = tokenizer.embedding_mat()\n",
    "        vocab_size, dim_emb = embedding.shape\n",
    "        self.embedding_module = nn.Embedding.from_pretrained(torch.FloatTensor(embedding), padding_idx=0, )\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_hid, 3, padding=1),\n",
    "            nn.BatchNorm1d(dim_hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(dim_hid, dim_hid, 3, padding=1),\n",
    "            nn.BatchNorm1d(dim_hid),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.seq2vec = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(len_seq * dim_hid, dim_hid),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.vec2seq = nn.Sequential(\n",
    "            nn.Linear(dim_hid, len_seq * dim_hid),\n",
    "            nn.ReLU(),\n",
    "            View(dim_hid, len_seq)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(dim_hid, dim_hid, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(dim_hid),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(dim_hid, dim_hid, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(dim_hid),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out_layer = nn.Linear(dim_hid, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.long()\n",
    "        seq_emb = self.embedding_module(inputs)\n",
    "        \n",
    "        seq_enc = self.encoder(seq_emb.transpose(1, 2))\n",
    "        vec = self.seq2vec(seq_enc)\n",
    "        seq_repr = self.vec2seq(vec)\n",
    "        indices = None\n",
    "        seq_dec = self.decoder(seq_repr)\n",
    "        out = self.out_layer(seq_dec.transpose(1, 2))\n",
    "        return out, seq_enc, vec, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "851293aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_with_loader(type_ = 'train',fold = None,  batch_size = 128):\n",
    "    if type_ != 'train' and type_ != 'val':\n",
    "#         data = pd.read_csv('../data/justina_test.csv')\n",
    "        data = pd.read_csv('./inputs/inputs_bd.csv')\n",
    "#         data = pd.read_csv('../data/test/GILGLVFTL.csv')\n",
    "#         data = pd.read_csv('../data/posi_length.csv')\n",
    "        \n",
    "    elif type_ == 'train':\n",
    "        data = pd.read_csv('./突变负样本/add_10xneg/add_10xneg/train_add10Xneg_{}.csv'.format(fold))\n",
    "\n",
    "    elif type_ == 'val':\n",
    "        data = pd.read_csv('./突变负样本/add_10xneg/add_10xneg/eva_add10Xneg_{}.csv'.format(fold))\n",
    "\n",
    "    pep_inputs, hla_inputs,labels = make_data(data)\n",
    "#     print(labels)\n",
    "    loader = Data.DataLoader(MyDataSet(pep_inputs, hla_inputs,labels), batch_size, shuffle = True, num_workers = 0)\n",
    "    n_samples = len(pep_inputs)\n",
    "    len_cdr3 = len(hla_inputs[0])\n",
    "    len_epi = len(pep_inputs[0])\n",
    "    encoding_mask = np.zeros([n_samples, len_cdr3,len_epi])\n",
    "    for idx_sample, (enc_cdr3_this, enc_epi_this) in enumerate(zip(hla_inputs, pep_inputs)):\n",
    "        mask = np.ones([len_cdr3,len_epi])\n",
    "        zero_cdr3 = (enc_cdr3_this == 0)\n",
    "        mask[zero_cdr3,:] = 0\n",
    "        zero_epi = (enc_epi_this == 0)\n",
    "        mask[:,zero_epi] = 0\n",
    "#         print(mask.shape)\n",
    "        encoding_mask[idx_sample] = mask\n",
    "    return data, pep_inputs, hla_inputs, labels,loader,encoding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f4a6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_with_loader(type_ = 'train',fold = None,  batch_size = 128):\n",
    "    if type_ != 'train' and type_ != 'val':\n",
    "#         data = pd.read_csv('../data/justina_test.csv')\n",
    "        data = pd.read_csv('./inputs/inputs_bd.csv')\n",
    "#         data = pd.read_csv('../data/test/GILGLVFTL.csv')\n",
    "#         data = pd.read_csv('../data/posi_length.csv')\n",
    "        \n",
    "    elif type_ == 'train':\n",
    "#         data = pd.read_csv('../new_train/train_VDJ_10X_McPAS_1V1_{}.csv'.format(fold))\n",
    "        data = pd.read_csv('./使用pMTnet数据训练/final_data/train_mismatch_{}.csv'.format(fold))\n",
    "\n",
    "    elif type_ == 'val':\n",
    "#         data = pd.read_csv('../new_train/eva_VDJ_10X_McPAS_1V1_{}.csv'.format(fold))\n",
    "        data = pd.read_csv('./使用pMTnet数据训练/final_data/eva_mismatch_{}.csv'.format(fold))\n",
    "\n",
    "    pep_inputs, hla_inputs,labels = make_data(data)\n",
    "#     print(labels)\n",
    "    loader = Data.DataLoader(MyDataSet(pep_inputs, hla_inputs,labels), batch_size, shuffle = True, num_workers = 0)\n",
    "    n_samples = len(pep_inputs)\n",
    "    len_cdr3 = len(hla_inputs[0])\n",
    "    len_epi = len(pep_inputs[0])\n",
    "    encoding_mask = np.zeros([n_samples, len_cdr3,len_epi])\n",
    "    for idx_sample, (enc_cdr3_this, enc_epi_this) in enumerate(zip(hla_inputs, pep_inputs)):\n",
    "        mask = np.ones([len_cdr3,len_epi])\n",
    "        zero_cdr3 = (enc_cdr3_this == 0)\n",
    "        mask[zero_cdr3,:] = 0\n",
    "        zero_epi = (enc_epi_this == 0)\n",
    "        mask[:,zero_epi] = 0\n",
    "#         print(mask.shape)\n",
    "        encoding_mask[idx_sample] = mask\n",
    "    return data, pep_inputs, hla_inputs, labels,loader,encoding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "205d7d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Fold-1=====\n",
      "-----Generate data loader-----\n",
      "Save fasta file to tmp_faketcr.fasta\n",
      " Aligning...\n",
      "zsh:1: command not found: ANARCI\n",
      "[[ 0 12  4 ... 12  0  0]\n",
      " [ 0  1  3 ... 16  0  0]\n",
      " [ 0 12  4 ... 12  0  0]\n",
      " ...\n",
      " [ 0 10  4 ...  3  0  0]\n",
      " [ 0 12  4 ...  3  0  0]\n",
      " [ 0  6 18 ...  2  0  0]]\n",
      "Save fasta file to tmp_faketcr.fasta\n",
      " Aligning...\n",
      "zsh:1: command not found: ANARCI\n",
      "[[ 0  6  4 ...  4  0  0]\n",
      " [ 0 12  4 ...  3  0  0]\n",
      " [ 0  0 20 ...  4  0  0]\n",
      " ...\n",
      " [ 0 12  4 ...  3  0  0]\n",
      " [ 0  1  4 ...  4  0  0]\n",
      " [ 0 12  4 ... 12  0  0]]\n",
      "Fold-1 Label info: Train = Counter({0: 56508, 1: 39168}) | Val = Counter({0: 14122, 1: 9798})\n",
      "-----Compile model-----\n",
      "-----Train-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-1/150 | Loss = 0.2527 | Time = 1.5878 sec\n",
      "tn = 50304, fp = 6204, fn = 3896, tp = 35272\n",
      "y_pred: 0 = 54200 | 1 = 41476\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9598|sensitivity=0.9005|specificity=0.8902|acc=0.8944|mcc=0.7846\n",
      "precision=0.8504|recall=0.9005|f1=0.8748|aupr=0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 39.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-1/150: Loss = 1.931552\n",
      "tn = 9881, fp = 4241, fn = 5210, tp = 4588\n",
      "y_pred: 0 = 15091 | 1 = 8829\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.6029|sensitivity=0.4683|specificity=0.6997|acc=0.6049|mcc=0.1711\n",
      "precision=0.5197|recall=0.4683|f1=0.4926|aupr=0.5216\n",
      "****Saving model: Best epoch = 1 | 5metrics_Best_avg = 0.4679\n",
      "*****Path saver:  ./tcr_st_layer1_multihead5_fold1_netmhcpan.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-2/150 | Loss = 0.1744 | Time = 1.5507 sec\n",
      "tn = 52787, fp = 3721, fn = 2545, tp = 36623\n",
      "y_pred: 0 = 55332 | 1 = 40344\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9798|sensitivity=0.9350|specificity=0.9342|acc=0.9345|mcc=0.8655\n",
      "precision=0.9078|recall=0.9350|f1=0.9212|aupr=0.9678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-2/150: Loss = 1.784450\n",
      "tn = 9946, fp = 4176, fn = 5207, tp = 4591\n",
      "y_pred: 0 = 15153 | 1 = 8767\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.6111|sensitivity=0.4686|specificity=0.7043|acc=0.6077|mcc=0.1764\n",
      "precision=0.5237|recall=0.4686|f1=0.4946|aupr=0.5257\n",
      "****Saving model: Best epoch = 2 | 5metrics_Best_avg = 0.4725\n",
      "*****Path saver:  ./tcr_st_layer1_multihead5_fold1_netmhcpan.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-3/150 | Loss = 0.1515 | Time = 1.5563 sec\n",
      "tn = 53109, fp = 3399, fn = 2143, tp = 37025\n",
      "y_pred: 0 = 55252 | 1 = 40424\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9847|sensitivity=0.9453|specificity=0.9398|acc=0.9421|mcc=0.8811\n",
      "precision=0.9159|recall=0.9453|f1=0.9304|aupr=0.9759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 30.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-3/150: Loss = 2.103628\n",
      "tn = 9803, fp = 4319, fn = 5183, tp = 4615\n",
      "y_pred: 0 = 14986 | 1 = 8934\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.6031|sensitivity=0.4710|specificity=0.6942|acc=0.6028|mcc=0.1679\n",
      "precision=0.5166|recall=0.4710|f1=0.4927|aupr=0.5201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-4/150 | Loss = 0.1379 | Time = 1.5416 sec\n",
      "tn = 53424, fp = 3084, fn = 1951, tp = 37217\n",
      "y_pred: 0 = 55375 | 1 = 40301\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9873|sensitivity=0.9502|specificity=0.9454|acc=0.9474|mcc=0.8919\n",
      "precision=0.9235|recall=0.9502|f1=0.9366|aupr=0.9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 35.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-4/150: Loss = 2.476070\n",
      "tn = 9916, fp = 4206, fn = 5239, tp = 4559\n",
      "y_pred: 0 = 15155 | 1 = 8765\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5980|sensitivity=0.4653|specificity=0.7022|acc=0.6051|mcc=0.1709\n",
      "precision=0.5201|recall=0.4653|f1=0.4912|aupr=0.5133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-5/150 | Loss = 0.1244 | Time = 1.5340 sec\n",
      "tn = 53707, fp = 2801, fn = 1794, tp = 37374\n",
      "y_pred: 0 = 55501 | 1 = 40175\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9897|sensitivity=0.9542|specificity=0.9504|acc=0.9520|mcc=0.9013\n",
      "precision=0.9303|recall=0.9542|f1=0.9421|aupr=0.9838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-5/150: Loss = 2.930256\n",
      "tn = 9748, fp = 4374, fn = 5198, tp = 4600\n",
      "y_pred: 0 = 14946 | 1 = 8974\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5943|sensitivity=0.4695|specificity=0.6903|acc=0.5998|mcc=0.1623\n",
      "precision=0.5126|recall=0.4695|f1=0.4901|aupr=0.5057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-6/150 | Loss = 0.1114 | Time = 1.5503 sec\n",
      "tn = 53921, fp = 2587, fn = 1647, tp = 37521\n",
      "y_pred: 0 = 55568 | 1 = 40108\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9917|sensitivity=0.9580|specificity=0.9542|acc=0.9557|mcc=0.9090\n",
      "precision=0.9355|recall=0.9580|f1=0.9466|aupr=0.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-6/150: Loss = 3.484137\n",
      "tn = 9788, fp = 4334, fn = 5226, tp = 4572\n",
      "y_pred: 0 = 15014 | 1 = 8906\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5941|sensitivity=0.4666|specificity=0.6931|acc=0.6003|mcc=0.1625\n",
      "precision=0.5134|recall=0.4666|f1=0.4889|aupr=0.5092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-7/150 | Loss = 0.1015 | Time = 1.5376 sec\n",
      "tn = 54131, fp = 2377, fn = 1484, tp = 37684\n",
      "y_pred: 0 = 55615 | 1 = 40061\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9932|sensitivity=0.9621|specificity=0.9579|acc=0.9596|mcc=0.9170\n",
      "precision=0.9407|recall=0.9621|f1=0.9513|aupr=0.9896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 30.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-7/150: Loss = 3.954656\n",
      "tn = 9781, fp = 4341, fn = 5270, tp = 4528\n",
      "y_pred: 0 = 15051 | 1 = 8869\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5824|sensitivity=0.4621|specificity=0.6926|acc=0.5982|mcc=0.1575\n",
      "precision=0.5105|recall=0.4621|f1=0.4851|aupr=0.4974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-8/150 | Loss = 0.0942 | Time = 1.4852 sec\n",
      "tn = 54344, fp = 2164, fn = 1437, tp = 37731\n",
      "y_pred: 0 = 55781 | 1 = 39895\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9941|sensitivity=0.9633|specificity=0.9617|acc=0.9624|mcc=0.9225\n",
      "precision=0.9458|recall=0.9633|f1=0.9545|aupr=0.9911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-8/150: Loss = 4.202820\n",
      "tn = 9554, fp = 4568, fn = 5216, tp = 4582\n",
      "y_pred: 0 = 14770 | 1 = 9150\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5762|sensitivity=0.4676|specificity=0.6765|acc=0.5910|mcc=0.1459\n",
      "precision=0.5008|recall=0.4676|f1=0.4836|aupr=0.4902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-9/150 | Loss = 0.0846 | Time = 1.4949 sec\n",
      "tn = 54562, fp = 1946, fn = 1331, tp = 37837\n",
      "y_pred: 0 = 55893 | 1 = 39783\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9953|sensitivity=0.9660|specificity=0.9656|acc=0.9657|mcc=0.9294\n",
      "precision=0.9511|recall=0.9660|f1=0.9585|aupr=0.9930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-9/150: Loss = 4.075807\n",
      "tn = 9459, fp = 4663, fn = 5087, tp = 4711\n",
      "y_pred: 0 = 14546 | 1 = 9374\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5835|sensitivity=0.4808|specificity=0.6698|acc=0.5924|mcc=0.1517\n",
      "precision=0.5026|recall=0.4808|f1=0.4914|aupr=0.4938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-10/150 | Loss = 0.0799 | Time = 1.5566 sec\n",
      "tn = 54660, fp = 1848, fn = 1262, tp = 37906\n",
      "y_pred: 0 = 55922 | 1 = 39754\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9958|sensitivity=0.9678|specificity=0.9673|acc=0.9675|mcc=0.9330\n",
      "precision=0.9535|recall=0.9678|f1=0.9606|aupr=0.9937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 30.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-10/150: Loss = 4.868225\n",
      "tn = 9963, fp = 4159, fn = 5307, tp = 4491\n",
      "y_pred: 0 = 15270 | 1 = 8650\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5789|sensitivity=0.4584|specificity=0.7055|acc=0.6043|mcc=0.1677\n",
      "precision=0.5192|recall=0.4584|f1=0.4869|aupr=0.4918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-11/150 | Loss = 0.0700 | Time = 1.4711 sec\n",
      "tn = 54883, fp = 1625, fn = 1076, tp = 38092\n",
      "y_pred: 0 = 55959 | 1 = 39717\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9968|sensitivity=0.9725|specificity=0.9712|acc=0.9718|mcc=0.9418\n",
      "precision=0.9591|recall=0.9725|f1=0.9658|aupr=0.9953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-11/150: Loss = 5.217054\n",
      "tn = 10049, fp = 4073, fn = 5402, tp = 4396\n",
      "y_pred: 0 = 15451 | 1 = 8469\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5809|sensitivity=0.4487|specificity=0.7116|acc=0.6039|mcc=0.1648\n",
      "precision=0.5191|recall=0.4487|f1=0.4813|aupr=0.4932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-12/150 | Loss = 0.0608 | Time = 1.5130 sec\n",
      "tn = 55123, fp = 1385, fn = 937, tp = 38231\n",
      "y_pred: 0 = 56060 | 1 = 39616\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9976|sensitivity=0.9761|specificity=0.9755|acc=0.9757|mcc=0.9499\n",
      "precision=0.9650|recall=0.9761|f1=0.9705|aupr=0.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 38.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-12/150: Loss = 5.287415\n",
      "tn = 9828, fp = 4294, fn = 5293, tp = 4505\n",
      "y_pred: 0 = 15121 | 1 = 8799\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5861|sensitivity=0.4598|specificity=0.6959|acc=0.5992|mcc=0.1588\n",
      "precision=0.5120|recall=0.4598|f1=0.4845|aupr=0.5025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-13/150 | Loss = 0.0551 | Time = 1.5372 sec\n",
      "tn = 55289, fp = 1219, fn = 897, tp = 38271\n",
      "y_pred: 0 = 56186 | 1 = 39490\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9980|sensitivity=0.9771|specificity=0.9784|acc=0.9779|mcc=0.9543\n",
      "precision=0.9691|recall=0.9771|f1=0.9731|aupr=0.9972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 34.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-13/150: Loss = 5.566410\n",
      "tn = 9903, fp = 4219, fn = 5227, tp = 4571\n",
      "y_pred: 0 = 15130 | 1 = 8790\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5934|sensitivity=0.4665|specificity=0.7012|acc=0.6051|mcc=0.1711\n",
      "precision=0.5200|recall=0.4665|f1=0.4918|aupr=0.5147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-14/150 | Loss = 0.0504 | Time = 1.5489 sec\n",
      "tn = 55396, fp = 1112, fn = 833, tp = 38335\n",
      "y_pred: 0 = 56229 | 1 = 39447\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9984|sensitivity=0.9787|specificity=0.9803|acc=0.9797|mcc=0.9580\n",
      "precision=0.9718|recall=0.9787|f1=0.9753|aupr=0.9977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 28.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-14/150: Loss = 6.298601\n",
      "tn = 9907, fp = 4215, fn = 5216, tp = 4582\n",
      "y_pred: 0 = 15123 | 1 = 8797\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5895|sensitivity=0.4676|specificity=0.7015|acc=0.6057|mcc=0.1725\n",
      "precision=0.5209|recall=0.4676|f1=0.4928|aupr=0.5252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-15/150 | Loss = 0.0462 | Time = 1.5723 sec\n",
      "tn = 55548, fp = 960, fn = 763, tp = 38405\n",
      "y_pred: 0 = 56311 | 1 = 39365\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9986|sensitivity=0.9805|specificity=0.9830|acc=0.9820|mcc=0.9628\n",
      "precision=0.9756|recall=0.9805|f1=0.9781|aupr=0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-15/150: Loss = 6.339417\n",
      "tn = 10171, fp = 3951, fn = 5354, tp = 4444\n",
      "y_pred: 0 = 15525 | 1 = 8395\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5898|sensitivity=0.4536|specificity=0.7202|acc=0.6110|mcc=0.1791\n",
      "precision=0.5294|recall=0.4536|f1=0.4885|aupr=0.5087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-16/150 | Loss = 0.0402 | Time = 1.5469 sec\n",
      "tn = 55677, fp = 831, fn = 679, tp = 38489\n",
      "y_pred: 0 = 56356 | 1 = 39320\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9990|sensitivity=0.9827|specificity=0.9853|acc=0.9842|mcc=0.9674\n",
      "precision=0.9789|recall=0.9827|f1=0.9808|aupr=0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-16/150: Loss = 6.091587\n",
      "tn = 10285, fp = 3837, fn = 5311, tp = 4487\n",
      "y_pred: 0 = 15596 | 1 = 8324\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5993|sensitivity=0.4580|specificity=0.7283|acc=0.6176|mcc=0.1923\n",
      "precision=0.5390|recall=0.4580|f1=0.4952|aupr=0.5215\n",
      "****Saving model: Best epoch = 16 | 5metrics_Best_avg = 0.4761\n",
      "*****Path saver:  ./tcr_st_layer1_multihead5_fold1_netmhcpan.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-17/150 | Loss = 0.0367 | Time = 1.5583 sec\n",
      "tn = 55801, fp = 707, fn = 605, tp = 38563\n",
      "y_pred: 0 = 56406 | 1 = 39270\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9991|sensitivity=0.9846|specificity=0.9875|acc=0.9863|mcc=0.9717\n",
      "precision=0.9820|recall=0.9846|f1=0.9833|aupr=0.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 38.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-17/150: Loss = 5.768851\n",
      "tn = 10105, fp = 4017, fn = 5270, tp = 4528\n",
      "y_pred: 0 = 15375 | 1 = 8545\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5959|sensitivity=0.4621|specificity=0.7156|acc=0.6117|mcc=0.1823\n",
      "precision=0.5299|recall=0.4621|f1=0.4937|aupr=0.5201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-18/150 | Loss = 0.0330 | Time = 1.5485 sec\n",
      "tn = 55830, fp = 678, fn = 565, tp = 38603\n",
      "y_pred: 0 = 56395 | 1 = 39281\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9993|sensitivity=0.9856|specificity=0.9880|acc=0.9870|mcc=0.9731\n",
      "precision=0.9827|recall=0.9856|f1=0.9842|aupr=0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 30.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-18/150: Loss = 6.344634\n",
      "tn = 9988, fp = 4134, fn = 5229, tp = 4569\n",
      "y_pred: 0 = 15217 | 1 = 8703\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5950|sensitivity=0.4663|specificity=0.7073|acc=0.6086|mcc=0.1774\n",
      "precision=0.5250|recall=0.4663|f1=0.4939|aupr=0.5253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-19/150 | Loss = 0.0275 | Time = 1.5498 sec\n",
      "tn = 55957, fp = 551, fn = 460, tp = 38708\n",
      "y_pred: 0 = 56417 | 1 = 39259\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9995|sensitivity=0.9883|specificity=0.9902|acc=0.9894|mcc=0.9782\n",
      "precision=0.9860|recall=0.9883|f1=0.9871|aupr=0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-19/150: Loss = 7.112957\n",
      "tn = 9740, fp = 4382, fn = 5111, tp = 4687\n",
      "y_pred: 0 = 14851 | 1 = 9069\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5906|sensitivity=0.4784|specificity=0.6897|acc=0.6031|mcc=0.1703\n",
      "precision=0.5168|recall=0.4784|f1=0.4968|aupr=0.5243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-20/150 | Loss = 0.0255 | Time = 1.5105 sec\n",
      "tn = 56011, fp = 497, fn = 424, tp = 38744\n",
      "y_pred: 0 = 56435 | 1 = 39241\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9996|sensitivity=0.9892|specificity=0.9912|acc=0.9904|mcc=0.9801\n",
      "precision=0.9873|recall=0.9892|f1=0.9883|aupr=0.9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-20/150: Loss = 7.905073\n",
      "tn = 9662, fp = 4460, fn = 5140, tp = 4658\n",
      "y_pred: 0 = 14802 | 1 = 9118\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5885|sensitivity=0.4754|specificity=0.6842|acc=0.5987|mcc=0.1616\n",
      "precision=0.5109|recall=0.4754|f1=0.4925|aupr=0.5382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-21/150 | Loss = 0.0225 | Time = 1.5516 sec\n",
      "tn = 56087, fp = 421, fn = 348, tp = 38820\n",
      "y_pred: 0 = 56435 | 1 = 39241\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9997|sensitivity=0.9911|specificity=0.9925|acc=0.9920|mcc=0.9834\n",
      "precision=0.9893|recall=0.9911|f1=0.9902|aupr=0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 29.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-21/150: Loss = 8.223892\n",
      "tn = 9551, fp = 4571, fn = 5071, tp = 4727\n",
      "y_pred: 0 = 14622 | 1 = 9298\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5823|sensitivity=0.4824|specificity=0.6763|acc=0.5969|mcc=0.1602\n",
      "precision=0.5084|recall=0.4824|f1=0.4951|aupr=0.5295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-22/150 | Loss = 0.0187 | Time = 1.5620 sec\n",
      "tn = 56158, fp = 350, fn = 290, tp = 38878\n",
      "y_pred: 0 = 56448 | 1 = 39228\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9998|sensitivity=0.9926|specificity=0.9938|acc=0.9933|mcc=0.9862\n",
      "precision=0.9911|recall=0.9926|f1=0.9918|aupr=0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-22/150: Loss = 8.301046\n",
      "tn = 9639, fp = 4483, fn = 5121, tp = 4677\n",
      "y_pred: 0 = 14760 | 1 = 9160\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5921|sensitivity=0.4773|specificity=0.6826|acc=0.5985|mcc=0.1618\n",
      "precision=0.5106|recall=0.4773|f1=0.4934|aupr=0.5361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-23/150 | Loss = 0.0200 | Time = 1.5472 sec\n",
      "tn = 56150, fp = 358, fn = 316, tp = 38852\n",
      "y_pred: 0 = 56466 | 1 = 39210\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9997|sensitivity=0.9919|specificity=0.9937|acc=0.9930|mcc=0.9854\n",
      "precision=0.9909|recall=0.9919|f1=0.9914|aupr=0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 34.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-23/150: Loss = 8.527172\n",
      "tn = 9804, fp = 4318, fn = 5145, tp = 4653\n",
      "y_pred: 0 = 14949 | 1 = 8971\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5876|sensitivity=0.4749|specificity=0.6942|acc=0.6044|mcc=0.1718\n",
      "precision=0.5187|recall=0.4749|f1=0.4958|aupr=0.5349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-24/150 | Loss = 0.0183 | Time = 1.4888 sec\n",
      "tn = 56174, fp = 334, fn = 278, tp = 38890\n",
      "y_pred: 0 = 56452 | 1 = 39224\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9998|sensitivity=0.9929|specificity=0.9941|acc=0.9936|mcc=0.9868\n",
      "precision=0.9915|recall=0.9929|f1=0.9922|aupr=0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-24/150: Loss = 8.173071\n",
      "tn = 9914, fp = 4208, fn = 5259, tp = 4539\n",
      "y_pred: 0 = 15173 | 1 = 8747\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5932|sensitivity=0.4633|specificity=0.7020|acc=0.6042|mcc=0.1688\n",
      "precision=0.5189|recall=0.4633|f1=0.4895|aupr=0.5326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-25/150 | Loss = 0.0171 | Time = 1.5290 sec\n",
      "tn = 56186, fp = 322, fn = 274, tp = 38894\n",
      "y_pred: 0 = 56460 | 1 = 39216\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9998|sensitivity=0.9930|specificity=0.9943|acc=0.9938|mcc=0.9871\n",
      "precision=0.9918|recall=0.9930|f1=0.9924|aupr=0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 31.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-25/150: Loss = 8.823400\n",
      "tn = 10026, fp = 4096, fn = 5349, tp = 4449\n",
      "y_pred: 0 = 15375 | 1 = 8545\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5894|sensitivity=0.4541|specificity=0.7100|acc=0.6051|mcc=0.1683\n",
      "precision=0.5207|recall=0.4541|f1=0.4851|aupr=0.5327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-26/150 | Loss = 0.0147 | Time = 1.5185 sec\n",
      "tn = 56251, fp = 257, fn = 236, tp = 38932\n",
      "y_pred: 0 = 56487 | 1 = 39189\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9940|specificity=0.9955|acc=0.9948|mcc=0.9893\n",
      "precision=0.9934|recall=0.9940|f1=0.9937|aupr=0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-26/150: Loss = 8.556540\n",
      "tn = 9902, fp = 4220, fn = 5297, tp = 4501\n",
      "y_pred: 0 = 15199 | 1 = 8721\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5780|sensitivity=0.4594|specificity=0.7012|acc=0.6021|mcc=0.1640\n",
      "precision=0.5161|recall=0.4594|f1=0.4861|aupr=0.5264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-27/150 | Loss = 0.0144 | Time = 1.5631 sec\n",
      "tn = 56254, fp = 254, fn = 223, tp = 38945\n",
      "y_pred: 0 = 56477 | 1 = 39199\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9943|specificity=0.9955|acc=0.9950|mcc=0.9897\n",
      "precision=0.9935|recall=0.9943|f1=0.9939|aupr=0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-27/150: Loss = 9.245920\n",
      "tn = 10018, fp = 4104, fn = 5325, tp = 4473\n",
      "y_pred: 0 = 15343 | 1 = 8577\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5906|sensitivity=0.4565|specificity=0.7094|acc=0.6058|mcc=0.1701\n",
      "precision=0.5215|recall=0.4565|f1=0.4869|aupr=0.5337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-28/150 | Loss = 0.0117 | Time = 1.5498 sec\n",
      "tn = 56322, fp = 186, fn = 180, tp = 38988\n",
      "y_pred: 0 = 56502 | 1 = 39174\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9954|specificity=0.9967|acc=0.9962|mcc=0.9921\n",
      "precision=0.9953|recall=0.9954|f1=0.9953|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-28/150: Loss = 9.153191\n",
      "tn = 9827, fp = 4295, fn = 5314, tp = 4484\n",
      "y_pred: 0 = 15141 | 1 = 8779\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5894|sensitivity=0.4576|specificity=0.6959|acc=0.5983|mcc=0.1566\n",
      "precision=0.5108|recall=0.4576|f1=0.4827|aupr=0.5306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-29/150 | Loss = 0.0106 | Time = 1.5653 sec\n",
      "tn = 56334, fp = 174, fn = 150, tp = 39018\n",
      "y_pred: 0 = 56484 | 1 = 39192\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9962|specificity=0.9969|acc=0.9966|mcc=0.9930\n",
      "precision=0.9956|recall=0.9962|f1=0.9959|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 28.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-29/150: Loss = 9.324862\n",
      "tn = 9777, fp = 4345, fn = 5192, tp = 4606\n",
      "y_pred: 0 = 14969 | 1 = 8951\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5909|sensitivity=0.4701|specificity=0.6923|acc=0.6013|mcc=0.1651\n",
      "precision=0.5146|recall=0.4701|f1=0.4913|aupr=0.5488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-30/150 | Loss = 0.0096 | Time = 1.4775 sec\n",
      "tn = 56344, fp = 164, fn = 149, tp = 39019\n",
      "y_pred: 0 = 56493 | 1 = 39183\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9962|specificity=0.9971|acc=0.9967|mcc=0.9932\n",
      "precision=0.9958|recall=0.9962|f1=0.9960|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 38.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-30/150: Loss = 10.199240\n",
      "tn = 9921, fp = 4201, fn = 5362, tp = 4436\n",
      "y_pred: 0 = 15283 | 1 = 8637\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5899|sensitivity=0.4527|specificity=0.7025|acc=0.6002|mcc=0.1590\n",
      "precision=0.5136|recall=0.4527|f1=0.4813|aupr=0.5416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-31/150 | Loss = 0.0107 | Time = 1.5207 sec\n",
      "tn = 56326, fp = 182, fn = 167, tp = 39001\n",
      "y_pred: 0 = 56493 | 1 = 39183\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9957|specificity=0.9968|acc=0.9964|mcc=0.9925\n",
      "precision=0.9954|recall=0.9957|f1=0.9955|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-31/150: Loss = 9.476794\n",
      "tn = 9880, fp = 4242, fn = 5257, tp = 4541\n",
      "y_pred: 0 = 15137 | 1 = 8783\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5902|sensitivity=0.4635|specificity=0.6996|acc=0.6029|mcc=0.1664\n",
      "precision=0.5170|recall=0.4635|f1=0.4888|aupr=0.5423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-32/150 | Loss = 0.0098 | Time = 1.4679 sec\n",
      "tn = 56327, fp = 181, fn = 151, tp = 39017\n",
      "y_pred: 0 = 56478 | 1 = 39198\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9961|specificity=0.9968|acc=0.9965|mcc=0.9928\n",
      "precision=0.9954|recall=0.9961|f1=0.9958|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 30.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-32/150: Loss = 9.346182\n",
      "tn = 10052, fp = 4070, fn = 5209, tp = 4589\n",
      "y_pred: 0 = 15261 | 1 = 8659\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.6083|sensitivity=0.4684|specificity=0.7118|acc=0.6121|mcc=0.1844\n",
      "precision=0.5300|recall=0.4684|f1=0.4973|aupr=0.5553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-33/150 | Loss = 0.0101 | Time = 1.5588 sec\n",
      "tn = 56336, fp = 172, fn = 170, tp = 38998\n",
      "y_pred: 0 = 56506 | 1 = 39170\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9957|specificity=0.9970|acc=0.9964|mcc=0.9926\n",
      "precision=0.9956|recall=0.9957|f1=0.9956|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 38.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-33/150: Loss = 8.921778\n",
      "tn = 9949, fp = 4173, fn = 5251, tp = 4547\n",
      "y_pred: 0 = 15200 | 1 = 8720\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5971|sensitivity=0.4641|specificity=0.7045|acc=0.6060|mcc=0.1722\n",
      "precision=0.5214|recall=0.4641|f1=0.4911|aupr=0.5449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-34/150 | Loss = 0.0102 | Time = 1.5248 sec\n",
      "tn = 56322, fp = 186, fn = 172, tp = 38996\n",
      "y_pred: 0 = 56494 | 1 = 39182\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9956|specificity=0.9967|acc=0.9963|mcc=0.9923\n",
      "precision=0.9953|recall=0.9956|f1=0.9954|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-34/150: Loss = 10.269807\n",
      "tn = 9849, fp = 4273, fn = 5278, tp = 4520\n",
      "y_pred: 0 = 15127 | 1 = 8793\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5938|sensitivity=0.4613|specificity=0.6974|acc=0.6007|mcc=0.1619\n",
      "precision=0.5140|recall=0.4613|f1=0.4863|aupr=0.5480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-35/150 | Loss = 0.0101 | Time = 1.5443 sec\n",
      "tn = 56323, fp = 185, fn = 165, tp = 39003\n",
      "y_pred: 0 = 56488 | 1 = 39188\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9958|specificity=0.9967|acc=0.9963|mcc=0.9924\n",
      "precision=0.9953|recall=0.9958|f1=0.9955|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-35/150: Loss = 10.566199\n",
      "tn = 9842, fp = 4280, fn = 5277, tp = 4521\n",
      "y_pred: 0 = 15119 | 1 = 8801\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5884|sensitivity=0.4614|specificity=0.6969|acc=0.6005|mcc=0.1615\n",
      "precision=0.5137|recall=0.4614|f1=0.4862|aupr=0.5411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-36/150 | Loss = 0.0117 | Time = 1.4945 sec\n",
      "tn = 56287, fp = 221, fn = 183, tp = 38985\n",
      "y_pred: 0 = 56470 | 1 = 39206\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9953|specificity=0.9961|acc=0.9958|mcc=0.9913\n",
      "precision=0.9944|recall=0.9953|f1=0.9948|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 31.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-36/150: Loss = 9.796679\n",
      "tn = 10013, fp = 4109, fn = 5420, tp = 4378\n",
      "y_pred: 0 = 15433 | 1 = 8487\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5871|sensitivity=0.4468|specificity=0.7090|acc=0.6016|mcc=0.1602\n",
      "precision=0.5158|recall=0.4468|f1=0.4789|aupr=0.5296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-37/150 | Loss = 0.0140 | Time = 1.4998 sec\n",
      "tn = 56240, fp = 268, fn = 222, tp = 38946\n",
      "y_pred: 0 = 56462 | 1 = 39214\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9943|specificity=0.9953|acc=0.9949|mcc=0.9894\n",
      "precision=0.9932|recall=0.9943|f1=0.9937|aupr=0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-37/150: Loss = 9.734117\n",
      "tn = 10064, fp = 4058, fn = 5479, tp = 4319\n",
      "y_pred: 0 = 15543 | 1 = 8377\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5962|sensitivity=0.4408|specificity=0.7126|acc=0.6013|mcc=0.1582\n",
      "precision=0.5156|recall=0.4408|f1=0.4753|aupr=0.5353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-38/150 | Loss = 0.0123 | Time = 1.4675 sec\n",
      "tn = 56278, fp = 230, fn = 196, tp = 38972\n",
      "y_pred: 0 = 56474 | 1 = 39202\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9950|specificity=0.9959|acc=0.9955|mcc=0.9908\n",
      "precision=0.9941|recall=0.9950|f1=0.9946|aupr=0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 35.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-38/150: Loss = 10.324287\n",
      "tn = 9941, fp = 4181, fn = 5179, tp = 4619\n",
      "y_pred: 0 = 15120 | 1 = 8800\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.6006|sensitivity=0.4714|specificity=0.7039|acc=0.6087|mcc=0.1788\n",
      "precision=0.5249|recall=0.4714|f1=0.4967|aupr=0.5626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-39/150 | Loss = 0.0095 | Time = 1.5439 sec\n",
      "tn = 56342, fp = 166, fn = 147, tp = 39021\n",
      "y_pred: 0 = 56489 | 1 = 39187\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9962|specificity=0.9971|acc=0.9967|mcc=0.9932\n",
      "precision=0.9958|recall=0.9962|f1=0.9960|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-39/150: Loss = 10.444735\n",
      "tn = 9923, fp = 4199, fn = 5235, tp = 4563\n",
      "y_pred: 0 = 15158 | 1 = 8762\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5928|sensitivity=0.4657|specificity=0.7027|acc=0.6056|mcc=0.1719\n",
      "precision=0.5208|recall=0.4657|f1=0.4917|aupr=0.5593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-40/150 | Loss = 0.0080 | Time = 1.5368 sec\n",
      "tn = 56367, fp = 141, fn = 130, tp = 39038\n",
      "y_pred: 0 = 56497 | 1 = 39179\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=1.0000|sensitivity=0.9967|specificity=0.9975|acc=0.9972|mcc=0.9941\n",
      "precision=0.9964|recall=0.9967|f1=0.9965|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 37.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-40/150: Loss = 10.625496\n",
      "tn = 9746, fp = 4376, fn = 5141, tp = 4657\n",
      "y_pred: 0 = 14887 | 1 = 9033\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.6041|sensitivity=0.4753|specificity=0.6901|acc=0.6021|mcc=0.1678\n",
      "precision=0.5156|recall=0.4753|f1=0.4946|aupr=0.5692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-41/150 | Loss = 0.0063 | Time = 1.5230 sec\n",
      "tn = 56390, fp = 118, fn = 86, tp = 39082\n",
      "y_pred: 0 = 56476 | 1 = 39200\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=1.0000|sensitivity=0.9978|specificity=0.9979|acc=0.9979|mcc=0.9956\n",
      "precision=0.9970|recall=0.9978|f1=0.9974|aupr=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 39.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-41/150: Loss = 11.666096\n",
      "tn = 9788, fp = 4334, fn = 5163, tp = 4635\n",
      "y_pred: 0 = 14951 | 1 = 8969\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5947|sensitivity=0.4731|specificity=0.6931|acc=0.6030|mcc=0.1688\n",
      "precision=0.5168|recall=0.4731|f1=0.4940|aupr=0.5669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 16.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-42/150 | Loss = 0.0057 | Time = 1.5085 sec\n",
      "tn = 56404, fp = 104, fn = 88, tp = 39080\n",
      "y_pred: 0 = 56492 | 1 = 39184\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=1.0000|sensitivity=0.9978|specificity=0.9982|acc=0.9980|mcc=0.9959\n",
      "precision=0.9973|recall=0.9978|f1=0.9975|aupr=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 35.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-42/150: Loss = 11.001197\n",
      "tn = 9961, fp = 4161, fn = 5246, tp = 4552\n",
      "y_pred: 0 = 15207 | 1 = 8713\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5953|sensitivity=0.4646|specificity=0.7054|acc=0.6067|mcc=0.1737\n",
      "precision=0.5224|recall=0.4646|f1=0.4918|aupr=0.5605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-43/150 | Loss = 0.0070 | Time = 1.5298 sec\n",
      "tn = 56382, fp = 126, fn = 120, tp = 39048\n",
      "y_pred: 0 = 56502 | 1 = 39174\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=1.0000|sensitivity=0.9969|specificity=0.9978|acc=0.9974|mcc=0.9947\n",
      "precision=0.9968|recall=0.9969|f1=0.9969|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 30.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-43/150: Loss = 10.228483\n",
      "tn = 9807, fp = 4315, fn = 5182, tp = 4616\n",
      "y_pred: 0 = 14989 | 1 = 8931\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.6075|sensitivity=0.4711|specificity=0.6944|acc=0.6030|mcc=0.1683\n",
      "precision=0.5169|recall=0.4711|f1=0.4929|aupr=0.5616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-44/150 | Loss = 0.0078 | Time = 1.5583 sec\n",
      "tn = 56368, fp = 140, fn = 121, tp = 39047\n",
      "y_pred: 0 = 56489 | 1 = 39187\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=1.0000|sensitivity=0.9969|specificity=0.9975|acc=0.9973|mcc=0.9944\n",
      "precision=0.9964|recall=0.9969|f1=0.9967|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-44/150: Loss = 12.157547\n",
      "tn = 9903, fp = 4219, fn = 5241, tp = 4557\n",
      "y_pred: 0 = 15144 | 1 = 8776\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5930|sensitivity=0.4651|specificity=0.7012|acc=0.6045|mcc=0.1697\n",
      "precision=0.5193|recall=0.4651|f1=0.4907|aupr=0.5606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:06<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1****Train (Ep avg): Epoch-45/150 | Loss = 0.0091 | Time = 1.5504 sec\n",
      "tn = 56364, fp = 144, fn = 143, tp = 39025\n",
      "y_pred: 0 = 56507 | 1 = 39169\n",
      "y_true: 0 = 56508 | 1 = 39168\n",
      "auc=0.9999|sensitivity=0.9963|specificity=0.9975|acc=0.9970|mcc=0.9938\n",
      "precision=0.9963|recall=0.9963|f1=0.9963|aupr=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 36.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 ****Test  Epoch-45/150: Loss = 10.641571\n",
      "tn = 9800, fp = 4322, fn = 5155, tp = 4643\n",
      "y_pred: 0 = 14955 | 1 = 8965\n",
      "y_true: 0 = 14122 | 1 = 9798\n",
      "auc=0.5999|sensitivity=0.4739|specificity=0.6940|acc=0.6038|mcc=0.1705\n",
      "precision=0.5179|recall=0.4739|f1=0.4949|aupr=0.5641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 83/94 [00:05<00:00, 16.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m time_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     ys_train, loss_train_list, metrics_train, time_train_ep \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# , dec_attns_train\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     ys_val, loss_val_list, metrics_val \u001b[38;5;241m=\u001b[39m eval_step(model, val_loader, fold, epoch, epochs, use_cuda) \u001b[38;5;66;03m#, dec_attns_val\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     metrics_ep_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(metrics_val[:\u001b[38;5;241m4\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, train_loader, fold, epoch, epochs, use_cuda)\u001b[0m\n\u001b[1;32m      6\u001b[0m y_true_train_list, y_prob_train_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m      7\u001b[0m loss_train_list, dec_attns_train_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_pep_inputs, train_hla_inputs, train_labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    pep_inputs: [batch_size, pep_len]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    hla_inputs: [batch_size, hla_len]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    train_outputs: [batch_size, 2]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     train_pep_inputs, train_hla_inputs, train_labels \u001b[38;5;241m=\u001b[39m train_pep_inputs\u001b[38;5;241m.\u001b[39mto(device), train_hla_inputs\u001b[38;5;241m.\u001b[39mto(device), train_labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/mistgpu/miniconda/envs/TranTCR/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/mistgpu/miniconda/envs/TranTCR/lib/python3.8/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mistgpu/miniconda/envs/TranTCR/lib/python3.8/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mistgpu/miniconda/envs/TranTCR/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mistgpu/miniconda/envs/TranTCR/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:170\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    168\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[1;32m    169\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melem_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "File \u001b[0;32m/mistgpu/miniconda/envs/TranTCR/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:170\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[1;32m    169\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n_heads in range(5,6):\n",
    "    \n",
    "    ys_train_fold_dict, ys_val_fold_dict = {}, {}\n",
    "    train_fold_metrics_list, val_fold_metrics_list = [], []\n",
    "    independent_fold_metrics_list, external_fold_metrics_list, ys_independent_fold_dict, ys_external_fold_dict = [], [], {}, {}\n",
    "    attns_train_fold_dict, attns_val_fold_dict, attns_independent_fold_dict, attns_external_fold_dict = {}, {}, {}, {}\n",
    "    loss_train_fold_dict, loss_val_fold_dict, loss_independent_fold_dict, loss_external_fold_dict = {}, {}, {}, {}\n",
    "\n",
    "    for fold in range(1,6):\n",
    "        print('=====Fold-{}====='.format(fold))\n",
    "        print('-----Generate data loader-----')\n",
    "        train_data, train_pep_inputs, train_hla_inputs, train_labels, train_loader,_ = data_with_loader(type_ = 'train', fold = fold,  batch_size = batch_size)\n",
    "        val_data, val_pep_inputs, val_hla_inputs, val_labels, val_loader,_ = data_with_loader(type_ = 'val', fold = fold,  batch_size = batch_size)\n",
    "        print('Fold-{} Label info: Train = {} | Val = {}'.format(fold, Counter(train_data.label), Counter(val_data.label)))\n",
    "\n",
    "        print('-----Compile model-----')\n",
    "        model = Transformer().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 1e-3)#, momentum = 0.99)\n",
    "\n",
    "        print('-----Train-----')\n",
    "        dir_saver = './model2'\n",
    "    \n",
    "        path_saver = './tcr_st_layer{}_multihead{}_fold{}_netmhcpan.pkl'.format(n_layers, n_heads, fold)\n",
    "        metric_best, ep_best = 0, -1\n",
    "        time_train = 0\n",
    "        for epoch in range(1, epochs + 1):\n",
    "\n",
    "            ys_train, loss_train_list, metrics_train, time_train_ep = train_step(model, train_loader, fold, epoch, epochs, use_cuda) # , dec_attns_train\n",
    "            ys_val, loss_val_list, metrics_val = eval_step(model, val_loader, fold, epoch, epochs, use_cuda) #, dec_attns_val\n",
    "\n",
    "            metrics_ep_avg = sum(metrics_val[:4])/4\n",
    "            if metrics_ep_avg > metric_best: \n",
    "                metric_best, ep_best = metrics_ep_avg, epoch\n",
    "                if not os.path.exists(dir_saver):\n",
    "                    os.makedirs(dir_saver)\n",
    "                print('****Saving model: Best epoch = {} | 5metrics_Best_avg = {:.4f}'.format(ep_best, metric_best))\n",
    "                print('*****Path saver: ', path_saver)\n",
    "                torch.save(model.eval().state_dict(), path_saver)\n",
    "\n",
    "            time_train += time_train_ep\n",
    "\n",
    "        print('-----Optimization Finished!-----')\n",
    "        print('-----Evaluate Results-----')\n",
    "        if ep_best >= 0:\n",
    "            print('*****Path saver: ', path_saver)\n",
    "            model.load_state_dict(torch.load(path_saver))\n",
    "            model_eval = model.eval()\n",
    "\n",
    "            ys_res_train, loss_res_train_list, metrics_res_train = eval_step(model_eval, train_loader, fold, ep_best, epochs, use_cuda) # , train_res_attns\n",
    "            ys_res_val, loss_res_val_list, metrics_res_val = eval_step(model_eval, val_loader, fold, ep_best, epochs, use_cuda) # , val_res_attns\n",
    "#             ys_res_independent, loss_res_independent_list, metrics_res_independent = eval_step(model_eval, independent_loader, fold, ep_best, epochs, use_cuda) # , independent_res_attns\n",
    "#             ys_res_external, loss_res_external_list, metrics_res_external = eval_step(model_eval, external_loader, fold, ep_best, epochs, use_cuda) # , external_res_attns\n",
    "\n",
    "            train_fold_metrics_list.append(metrics_res_train)\n",
    "            val_fold_metrics_list.append(metrics_res_val)\n",
    "#             independent_fold_metrics_list.append(metrics_res_independent)\n",
    "#             external_fold_metrics_list.append(metrics_res_external)\n",
    "\n",
    "#             ys_train_fold_dict[fold], ys_val_fold_dict[fold], ys_independent_fold_dict[fold], ys_external_fold_dict[fold] = ys_res_train, ys_res_val, ys_res_independent, ys_res_external    \n",
    "#             attns_train_fold_dict[fold], attns_val_fold_dict[fold], attns_independent_fold_dict[fold], attns_external_fold_dict[fold] = train_res_attns, val_res_attns, independent_res_attns, external_res_attns   \n",
    "#             loss_train_fold_dict[fold], loss_val_fold_dict[fold], loss_independent_fold_dict[fold], loss_external_fold_dict[fold] = loss_res_train_list, loss_res_val_list, loss_res_independent_list, loss_res_external_list  \n",
    "\n",
    "        print(\"Total training time: {:6.2f} sec\".format(time_train))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_heads in range(5,10):\n",
    "    for fold in range(1,6):\n",
    "\n",
    "        path_saver = './VDJ_10X_McPAS_随机错配_1V5/tcr_st_layer1_multihead{}_fold{}_netmhcpan.pkl'.format(n_heads,fold)\n",
    "#         path_saver = 'model/tcr_st_layer1_multihead8_fold0_netmhcpan.pkl'\n",
    "# model = STSeqCls((21, 100), num_cls=2, hidden_size=300, num_layers=1, num_head=8, max_len=29,cls_hidden_size=600,dropout=0.1,head_dim=32).to(device)\n",
    "        model = Transformer().to(device)\n",
    "        model.load_state_dict(torch.load(path_saver))\n",
    "# model_eval = model.eval()\n",
    "        type_ = 'val'\n",
    "        save_ = False\n",
    "        use_cuda = True\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # fold = 0\n",
    "        ep_best = None\n",
    "        print(\"n_head is:\"+str(n_heads))\n",
    "\n",
    "        data, pep_inputs, hla_inputs, labels, loader,_ = data_with_loader(type_,fold = fold,  batch_size = batch_size)\n",
    "        independent_metrics_res, independent_ys_res, independent_attn_res = eval_step(model, loader, fold, ep_best, epochs, use_cuda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908d0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_with_loader(type_ = 'train',fold = None,  batch_size = 128):\n",
    "    if type_ != 'train' and type_ != 'val':\n",
    "#         data = pd.read_csv('../data/justina_test.csv')\n",
    "        data = pd.read_csv('./inputs/inputs_bd.csv')\n",
    "#         data = pd.read_csv('../data/test/GILGLVFTL.csv')\n",
    "#         data = pd.read_csv('../data/posi_length.csv')\n",
    "        \n",
    "    elif type_ == 'train':\n",
    "        data = pd.read_csv('./Combine_10Xneg/Com_train_B.csv')\n",
    "\n",
    "    elif type_ == 'val':\n",
    "        data = pd.read_csv('./Combine_10Xneg/Com_eva_test_B.csv')\n",
    "\n",
    "    pep_inputs, hla_inputs,labels = make_data(data)\n",
    "#     print(labels)\n",
    "    loader = Data.DataLoader(MyDataSet(pep_inputs, hla_inputs,labels), batch_size, shuffle = True, num_workers = 0)\n",
    "    n_samples = len(pep_inputs)\n",
    "    len_cdr3 = len(hla_inputs[0])\n",
    "    len_epi = len(pep_inputs[0])\n",
    "    encoding_mask = np.zeros([n_samples, len_cdr3,len_epi])\n",
    "    for idx_sample, (enc_cdr3_this, enc_epi_this) in enumerate(zip(hla_inputs, pep_inputs)):\n",
    "        mask = np.ones([len_cdr3,len_epi])\n",
    "        zero_cdr3 = (enc_cdr3_this == 0)\n",
    "        mask[zero_cdr3,:] = 0\n",
    "        zero_epi = (enc_epi_this == 0)\n",
    "        mask[:,zero_epi] = 0\n",
    "#         print(mask.shape)\n",
    "        encoding_mask[idx_sample] = mask\n",
    "    return data, pep_inputs, hla_inputs, labels,loader,encoding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71cf16cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Fold-1=====\n",
      "-----Generate data loader-----\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The cdr3 length must <= 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_57767/1036963441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=====Fold-{}====='\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-----Generate data loader-----'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pep_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hla_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_with_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pep_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_hla_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_with_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fold-{} Label info: Train = {} | Val = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_57767/2575137166.py\u001b[0m in \u001b[0;36mdata_with_loader\u001b[0;34m(type_, fold, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./使用pMTnet数据训练/final_data/eva_mismatch_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpep_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhla_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#     print(labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpep_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhla_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_57767/3703077352.py\u001b[0m in \u001b[0;36mmake_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhla_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_cdr3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdr3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#     print(hla_inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpep_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_epi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepitope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_57767/2755319269.py\u001b[0m in \u001b[0;36mencode_cdr3\u001b[0;34m(cdr3, tokenizer)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mlen_cdr3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcdr3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mmax_len_cdr3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_cdr3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mmax_len_cdr3\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The cdr3 length must <= 20'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mmax_len_cdr3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The cdr3 length must <= 20"
     ]
    }
   ],
   "source": [
    "for n_heads in range(5,6):\n",
    "    \n",
    "    ys_train_fold_dict, ys_val_fold_dict = {}, {}\n",
    "    train_fold_metrics_list, val_fold_metrics_list = [], []\n",
    "    independent_fold_metrics_list, external_fold_metrics_list, ys_independent_fold_dict, ys_external_fold_dict = [], [], {}, {}\n",
    "    attns_train_fold_dict, attns_val_fold_dict, attns_independent_fold_dict, attns_external_fold_dict = {}, {}, {}, {}\n",
    "    loss_train_fold_dict, loss_val_fold_dict, loss_independent_fold_dict, loss_external_fold_dict = {}, {}, {}, {}\n",
    "\n",
    "    for fold in range(1,6):\n",
    "        print('=====Fold-{}====='.format(fold))\n",
    "        print('-----Generate data loader-----')\n",
    "        train_data, train_pep_inputs, train_hla_inputs, train_labels, train_loader,_ = data_with_loader(type_ = 'train', fold = fold,  batch_size = batch_size)\n",
    "        val_data, val_pep_inputs, val_hla_inputs, val_labels, val_loader,_ = data_with_loader(type_ = 'val', fold = fold,  batch_size = batch_size)\n",
    "        print('Fold-{} Label info: Train = {} | Val = {}'.format(fold, Counter(train_data.label), Counter(val_data.label)))\n",
    "\n",
    "        print('-----Compile model-----')\n",
    "        model = Transformer().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 1e-3)#, momentum = 0.99)\n",
    "\n",
    "        print('-----Train-----')\n",
    "        dir_saver = './model2'\n",
    "    \n",
    "        path_saver = './model2/tcr_st_layer{}_multihead{}_fold{}_netmhcpan.pkl'.format(n_layers, n_heads, fold)\n",
    "        metric_best, ep_best = 0, -1\n",
    "        time_train = 0\n",
    "        for epoch in range(1, epochs + 1):\n",
    "\n",
    "            ys_train, loss_train_list, metrics_train, time_train_ep = train_step(model, train_loader, fold, epoch, epochs, use_cuda) # , dec_attns_train\n",
    "            ys_val, loss_val_list, metrics_val = eval_step(model, val_loader, fold, epoch, epochs, use_cuda) #, dec_attns_val\n",
    "\n",
    "            metrics_ep_avg = sum(metrics_val[:4])/4\n",
    "            if metrics_ep_avg > metric_best: \n",
    "                metric_best, ep_best = metrics_ep_avg, epoch\n",
    "                if not os.path.exists(dir_saver):\n",
    "                    os.makedirs(dir_saver)\n",
    "                print('****Saving model: Best epoch = {} | 5metrics_Best_avg = {:.4f}'.format(ep_best, metric_best))\n",
    "                print('*****Path saver: ', path_saver)\n",
    "                torch.save(model.eval().state_dict(), path_saver)\n",
    "\n",
    "            time_train += time_train_ep\n",
    "\n",
    "        print('-----Optimization Finished!-----')\n",
    "        print('-----Evaluate Results-----')\n",
    "        if ep_best >= 0:\n",
    "            print('*****Path saver: ', path_saver)\n",
    "            model.load_state_dict(torch.load(path_saver))\n",
    "            model_eval = model.eval()\n",
    "\n",
    "            ys_res_train, loss_res_train_list, metrics_res_train = eval_step(model_eval, train_loader, fold, ep_best, epochs, use_cuda) # , train_res_attns\n",
    "            ys_res_val, loss_res_val_list, metrics_res_val = eval_step(model_eval, val_loader, fold, ep_best, epochs, use_cuda) # , val_res_attns\n",
    "#             ys_res_independent, loss_res_independent_list, metrics_res_independent = eval_step(model_eval, independent_loader, fold, ep_best, epochs, use_cuda) # , independent_res_attns\n",
    "#             ys_res_external, loss_res_external_list, metrics_res_external = eval_step(model_eval, external_loader, fold, ep_best, epochs, use_cuda) # , external_res_attns\n",
    "\n",
    "            train_fold_metrics_list.append(metrics_res_train)\n",
    "            val_fold_metrics_list.append(metrics_res_val)\n",
    "#             independent_fold_metrics_list.append(metrics_res_independent)\n",
    "#             external_fold_metrics_list.append(metrics_res_external)\n",
    "\n",
    "#             ys_train_fold_dict[fold], ys_val_fold_dict[fold], ys_independent_fold_dict[fold], ys_external_fold_dict[fold] = ys_res_train, ys_res_val, ys_res_independent, ys_res_external    \n",
    "#             attns_train_fold_dict[fold], attns_val_fold_dict[fold], attns_independent_fold_dict[fold], attns_external_fold_dict[fold] = train_res_attns, val_res_attns, independent_res_attns, external_res_attns   \n",
    "#             loss_train_fold_dict[fold], loss_val_fold_dict[fold], loss_independent_fold_dict[fold], loss_external_fold_dict[fold] = loss_res_train_list, loss_res_val_list, loss_res_independent_list, loss_res_external_list  \n",
    "\n",
    "        print(\"Total training time: {:6.2f} sec\".format(time_train))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TranTCR",
   "language": "python",
   "name": "trantcr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
